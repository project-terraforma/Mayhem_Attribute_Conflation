{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mayhem Project - Attribute Conflation Pipeline (Colab Setup)\n",
    "\n",
    "This notebook guides you through setting up and running the attribute conflation pipeline for the Mayhem project in Google Colab.\n",
    "It covers cloning the repository, installing dependencies, and running the full pipeline for a single attribute (`name`) as a demonstration.\n",
    "You can extend this to run for all attributes and further experiments.\n",
    "\n",
    "## 1. Clone the Repository\n",
    "\n",
    "First, we'll clone your GitHub repository. Make sure you have the correct URL.\n",
    "If your repository is private, you might need to generate a Personal Access Token (PAT) from GitHub and use it in the URL (e.g., `https://<PAT>@github.com/your-username/your-repo.git`).\n",
    "\n",
    "Replace `<YOUR_REPO_URL>` with the actual URL of your Mayhem project repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual repository URL\n",
    "REPO_URL = \"https://github.com/project-terraforma/Mayhem_Attribute_Conflation.git\"\n",
    "REPO_NAME = REPO_URL.split('/')[-1].replace('.git', '')\n",
    "\n",
    "!git clone {REPO_URL}\n",
    "%cd {REPO_NAME}\n",
    "\n",
    "# IMPORTANT: Checkout your working branch to get all the latest changes\n",
    "print(f\"Checking out branch: local_model_annotation\")\n",
    "!git checkout local_model_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Next, we'll install all the necessary Python packages listed in your `requirements.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Pipeline for 'name' Attribute\n",
    "\n",
    "This section will demonstrate running the full pipeline for the 'name' attribute:\n",
    "1.  Generate synthetic training data from Yelp.\n",
    "2.  Process this synthetic data into features.\n",
    "3.  Train ML models.\n",
    "4.  Evaluate the model on your 200 real-world records.\n",
    "5.  Run inference on the 2,000 Overture records.\n",
    "\n",
    "This will also log inference time and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTE = 'name'\n",
    "\n",
    "print(f\"\\n--- Running pipeline for attribute: {ATTRIBUTE} ---\")\n",
    "\n",
    "# 1. Generate synthetic dataset\n",
    "!python scripts/generate_synthetic_dataset.py\n",
    "\n",
    "# 2. Process synthetic data into features\n",
    "!python -m scripts.process_synthetic_data --attribute {ATTRIBUTE}\n",
    "\n",
    "# 3. Train ML models\n",
    "!python scripts/train_models.py --features data/processed/features_{ATTRIBUTE}_synthetic.parquet --output-dir models/ml_models/{ATTRIBUTE}\n",
    "\n",
    "# 4. Evaluate on real-world data (200 records)\n",
    "!python -m scripts.evaluate_real_data --attribute {ATTRIBUTE} --model models/ml_models/{ATTRIBUTE}/best_model_gradient_boosting.joblib\n",
    "\n",
    "# 5. Run inference on 2,000 Overture records\\n",
    "!python -m scripts.run_inference --attribute {ATTRIBUTE} --model models/ml_models/{ATTRIBUTE}/best_model_gradient_boosting.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Results\n",
    "\n",
    "You can now find the generated files in your repository structure:\n",
    "*   `data/synthetic_golden_dataset_2k.json` (The synthetic training data)\n",
    "*   `data/processed/features_name_synthetic.parquet` (Extracted features)\n",
    "*   `models/ml_models/name/` (Trained model and summary)\n",
    "*   `data/results/final_evaluation_report.txt` (Evaluation on 200 real records)\n",
    "*   `data/results/final_conflated_names_2k.json` (Inference results on 2,000 Overture records)\n",
    "\n",
    "You can also commit these changes back to your GitHub repository from Colab if you configure git credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "To run for all attributes (phone, website, address, category):\n",
    "1.  Copy and paste the code block from section 3.\n",
    "2.  Change `ATTRIBUTE = 'name'` to `ATTRIBUTE = 'phone'`, then `ATTRIBUTE = 'website'`, etc.\n",
    "3.  After running for all attributes, run the consolidation script:\n",
    "    ```bash\n",
    "    !python scripts/consolidate_results.py\n",
    "    ```\n",
    "4.  **Scaling and Refinement:**\n",
    "    *   Modify `scripts/generate_synthetic_dataset.py` to generate a larger dataset (e.g., 10,000 or 50,000 records) to leverage the full Yelp dataset.\n",
    "    *   Experiment with different ML models (Varnit's `scripts/train_models.py` already supports Logistic Regression, Random Forest, Gradient Boosting).\n",
    "    *   Incorporate rule-based baselines (`scripts/baseline_heuristics.py` from Varnit's branch) and compare performance.\n",
    "    *   Perform error analysis (OKR 2, KR2) on misclassified records from `scripts/evaluate_real_data.py` to identify patterns and refine features or rules.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": "form",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}